# -*- coding: utf-8 -*-
"""single_cell_pbmc_kmeans_knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WrLXcig1pTIRlmhEo5aXAlUXxyLOfFCi

**BMEN4480**

**Assignment 2**

**Michelle Campoli (mec2308)**
"""

# MOUNTING GOOGLE DRIVE WHERE DATA IS STORED
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

#Imports
import gc
import os

import shutil
import tarfile
import numpy as np
import pandas as pd
!pip install scanpy
!pip3 install leidenalg
!pip3 install louvain
import scanpy as sc
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.io as sio
from scipy.io import mmread
import sklearn
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.neighbors import KNeighborsClassifier

sc.set_figure_params(figsize=(6, 6))
sc.settings.verbosity = 3

# open file
directory = '/gdrive/My Drive/Statistical ML/Homework 2/'
filename = "pbmc_6Kcells_500genes.txt"
adata = sc.read_text(directory+filename)
print(adata)

# add metrics to dataframe
sc.pp.calculate_qc_metrics(adata, inplace= True)

# Exploring data
# cell data
adata.obs.head()

adata.obs_names

# gene data
adata.var.head()

adata.var_names

# data output from file
adata.X

# more understanding of dataframe
adata.to_df().head()

"""# **Question 1.a.**
Normalize the data by scaling to median library size and log transform the normalized data and perform PCA, followed by t-SNE or UMAP on the top 20 PCs. Plot the 2D or 3D embedding.
"""

# normalize data
adata.layers["counts"] = adata.X.copy() #preserve counts
sc.pp.normalize_total(adata, target_sum=None)

# log transform
sc.pp.log1p(adata)
adata.raw = adata

# PCA -- 2 principal components
sc.tl.pca(adata, n_comps = 2)
sc.pl.pca(adata)

# variance ratios
adata.uns['pca']['variance_ratio']

# total variance accounted for with 2 principal components
adata.uns['pca']['variance_ratio'].sum()

"""The Variance ratios are [0.23252793, 0.12895969] for principal component 1 and 2, respectively."""

# PCA -- 20 principal components
sc.tl.pca(adata, n_comps = 20)

sc.pl.pca(adata, components="all")

# variance ratios
adata.uns['pca']['variance_ratio']

# total variance accounted for with 20 principal components
adata.uns['pca']['variance_ratio'].sum()

"""The Variance Ratios are [0.23252808, 0.12895963, 0.04903975, 0.03065705, 0.00844455, 0.00735999, 0.00647852, 0.0053518 , 0.0044673 , 0.00418035, 0.00402829, 0.00392926, 0.00380462, 0.00376503, 0.00375513, 0.00372105, 0.00365731, 0.00360696, 0.00359425, 0.00354582] for all 20 principal components."""

# UMAP
sc.pp.neighbors(adata, n_pcs= 20)
sc.tl.umap(adata)

sc.pl.umap(adata)

# t-SNE
sc.tl.tsne(adata, n_pcs= 20)
sc.pl.tsne(adata)

"""# **Question 1.b.**
Is 20 a good choice for the number of PCs?
"""

# PCA-- 3 principal components
sc.tl.pca(adata, n_comps = 3)
sc.pl.pca(adata, components="all")
adata.uns['pca']['variance_ratio']

# total variance accounted for with 3 PCs
adata.uns['pca']['variance_ratio'].sum()

"""PCA is used to reduce the number of dimensions (features) in a dataset. Above, I plotted the first two principal components, 20 principal components, and 3 principal components. Finding clusters in the dataset is the goal, and using 2 principal components split the dataset vaguely into 2 or 3 clusters. The sum of the variance ratios for 2 principal components is 0.3615. This means with only the first two principal components, 36% of the variance in the data was captured. 

The sum for 20 is 0.5148747. This means with 20 principal components, 51% of the variance in the data was captured. Looking at the array of variance ratios for each principal component:

[0.23252808, 0.12895963, 0.04903975, 0.03065705, 0.00844455,
       0.00735999, 0.00647852, 0.0053518 , 0.0044673 , 0.00418035,
       0.00402829, 0.00392926, 0.00380462, 0.00376503, 0.00375513,
       0.00372105, 0.00365731, 0.00360696, 0.00359425, 0.00354582]

We can see that up until the 4th PC, most of the variance is captured. Then the remaining PC's 5-20 are picking up on very little variance (<0.8%). 

Then when looking at 3 principal component variance ratios, they sum to 0.41052738. So 41% of the variance in the dataset is captured by 3 principal components. The difference between 3 PCs and 20 PCs is only 10%. 

Therefore, 20 principal components is not a good choice for the number of PCs. It is overkill because a 20-D graph is not easily interpretable and the computations are much more expensive for only a small increase in the capture of variance. Working with 3 or 4 PCs is enough.

# **Question 2.**
Cluster cells using the K-means method and color the embedding from Q1 with cluster IDs. Justify your choice of K.
"""

# where is pca, umap, and tsne data stored: data.obsm
adata

# Cluster cells using K-means method 

#X_pca = adata.obsm['X_pca']
X_umap = adata.obsm['X_umap']
#X_tsne = adata.obsm['X_tsne']

# kmeans with k= 2
kmeans = KMeans(init= "k-means++", n_clusters=2, random_state=0).fit(X_umap) 
adata.obs['kmeans2'] = kmeans.labels_.astype(str)

# kmeans with k= 5
kmeans = KMeans(n_clusters=5, random_state=0).fit(X_umap) 
adata.obs['kmeans5'] = kmeans.labels_.astype(str)

# kmeans with k= 10
kmeans = KMeans(n_clusters=10, random_state=0).fit(X_umap) 
adata.obs['kmeans10'] = kmeans.labels_.astype(str)

sc.pl.umap(adata, color=['kmeans2', 'kmeans5', 'kmeans10'])

"""My choice of K would be k=2. There are two major groups visible on the umap that can be two distinct clusters.

# **Question 3**
Compute a 30-NN (nearest neighbor) graph between cells. Plot a heatmap of the adjacency matrix for the graph. Justify your distance metric.
"""

# 30-KNN graph, cosine distance
sc.tl.pca(adata, n_comps = 20)
sc.pp.neighbors(adata, n_neighbors= 30, n_pcs= 20, knn= True, metric= 'cosine')
sc.tl.leiden(adata)
sc.pl.umap(adata, color= ['leiden'])

# 30-KNN graph, manhattan distance
sc.pp.neighbors(adata, n_neighbors= 30, n_pcs= 20, knn= True, metric= 'manhattan')
sc.tl.leiden(adata)
sc.pl.umap(adata, color= ['leiden'])

# 30-KNN graph, euclidean distance
sc.pp.neighbors(adata, n_neighbors= 30, n_pcs= 20, knn= True, metric= 'euclidean')
sc.tl.leiden(adata)
sc.pl.umap(adata, color= ['leiden'])

# Heatmap of adjacency matrix
matrix = adata.obsp['connectivities']
heat_map = matrix[:,:].todense()
plt.imshow(heat_map, cmap= 'viridis')
plt.show()

"""I decided to go with the euclidean distance metric because clusters were the most well defined.

# **Question 4**
Cluster cells using a graph-based algorithm such as Louvain with the kNN graph from Q3. Color the embedding with cluster IDs. How does it compare to K-means?
"""

sc.pp.neighbors(adata, n_neighbors= 30, n_pcs= 20, knn= True, metric= 'euclidean')
sc.tl.louvain(adata, key_added = "louvain_1.0") # default resolution in 1.0
sc.tl.louvain(adata, resolution = 0.6, key_added = "louvain_0.6")
sc.tl.louvain(adata, resolution = 0.4, key_added = "louvain_0.4")
sc.tl.louvain(adata, resolution = 1.4, key_added = "louvain_1.4")

sc.pl.umap(adata, color=['louvain_0.4', 'louvain_0.6', 'louvain_1.0','louvain_1.4'])

"""For louvain clustering, higher resolution means finding more and smaller clusters. The default, resolution= 1.0, found 7 clusters. At louvain standard resolution there are some clusters that contain datapoints from other clusters, such as purple&blue. Compared to k means clustering, the clusters are split differently. At low resolution louvain did not separate the two visibily obvious clusters well, meanwhile k means did (into two clusters, left and right). The first louvain graph shows three clusters and the green&orange clusters overlap as well as the blue&green clusters. Therefore I would say k means worked better at defining clusters since there were no datapoints mixed between clusters, at least from visually looking at the graph.

# **Question 5**

**Question 5.a.**
Perform a t-test to find differentially expressed genes (DEGs) in a cluster of your choice. Color the embedding with the expression of the top 5 DEGs.
"""

# Perform a t-test to find differentially expressed genes (DEGs) in a cluster of your choice.
# T-test performed on 30-KNN graph, euclidean distance, using leiden. There are 9 clusters total. This test compares each cluster to the rest.
sc.tl.rank_genes_groups(adata, 'leiden', method='t-test')
sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)

"""In these t-test plots, the higher the score the more dissimilar the genes in the cluster of choice are from all the other clusters. The highest score appears in cluster 2. The top 5 DEGs in this cluster are 48, 4231, 1109, 4000, and 267."""

# 30-KNN graph, euclidean distance
sc.pp.neighbors(adata, n_neighbors= 30, n_pcs= 20, knn= True, metric= 'euclidean')
sc.tl.leiden(adata)
sc.pl.umap(adata, color= ['48', '4231', '1109', '4000', '267'])

"""**Question 5. b.**
How would you characterize the cluster?

From the scores of the t-test, cluster 2 is most dissimilar based on gene expression compared to the rest of the clusters. The t-test scores were highest for the top 25 genes in cluster 2 (with scores ranging from 22 to 28). For clusters with scores close to 0, this means the genes expressed in those clusters are similar to other clusters.

**Question 5.c.**
Which other differential expression method might be appropriate for the expression distribution in this data?
"""

sc.tl.rank_genes_groups(adata, 'leiden', method='wilcoxon')
sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)

"""Using a Wilcoxon rank sum test might be appropriate for the expression distribution in this data. The wilcoxon is a non-parametric test and does not assume a normally distributed dataset. It is based on ranks rather than on actual values. It is more appropriate when data is not normally distributed. We can see from these plots that scores are similar but less extreme than the t-tests. """